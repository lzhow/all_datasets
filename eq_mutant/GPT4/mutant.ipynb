{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"GPT4/mutant/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# \n",
    "file_few = pd.read_excel(\"codellama_fewshot_predictions.xlsx\")\n",
    "\n",
    "questions_few = file_few['question']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# \n",
    "file = pd.read_excel(\"codellama_zeroshot_predictions.xlsx\")\n",
    "\n",
    "questions = file['question']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict  isCorrect\n",
    "file_few['predict'] = \"\"\n",
    "file_few['isCorrect'] = \"\"\n",
    "\n",
    "# \n",
    "file.to_excel(\"codellama_fewshot_predictions.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict  isCorrect\n",
    "file['predict'] = \"\"\n",
    "file['isCorrect'] = \"\"\n",
    "\n",
    "# \n",
    "file.to_excel(\"codellama_zeroshot_predictions.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "def load_plm(model_name):\n",
    "  # AutoModelForMaskedLM\n",
    "  tokenizer = AutoTokenizer.from_pretrained(f'huggingface/hub/{model_name}', trust_remote_code=True)\n",
    "  model = AutoModelForCausalLM.from_pretrained(f'huggingface/hub/{model_name}', trust_remote_code=True)\n",
    "\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  model.to(device)\n",
    "  return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_plm('CodeLlama-13b-Instruct-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question):\n",
    "  input_ids = tokenizer.encode(question, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "\n",
    "  outputs = model.generate(input_ids, max_new_tokens=256, do_sample=False, top_p=0.9, no_repeat_ngram_size=2)\n",
    "  response = tokenizer.decode(outputs[0])\n",
    "\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# \n",
    "chat_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# \n",
    "def chat_with_model(question):\n",
    "    prompt_template = \"\"\" \n",
    "# <s>[INST] \n",
    "# {query}\n",
    "# [/INST]\n",
    "#  \n",
    "\"\"\"\n",
    "    \n",
    "    prompt = prompt_template.format(query=question)\n",
    "    input_ids = chat_pipe.tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
    "    response = chat_pipe.model.generate(input_ids, max_new_tokens=128, no_repeat_ngram_size=2)\n",
    "    return chat_pipe.tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "\n",
    "# # \n",
    "# prompt = \" Python ï¼Ÿ\"  # \n",
    "# print(chat_with_model(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def get_answer(question):\n",
    "  pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "   # \n",
    "  prompt_template = \"\"\" \n",
    "<s>[INST] <<SYS>>\n",
    "You are a smart Mutant Test analyser. You should pay attention on the difference of the output.\\n\n",
    "<</SYS>>\n",
    "{query}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "  prompt = prompt_template.format(query=question)\n",
    "\n",
    "  outputs = pipe(prompt, max_new_tokens=256, no_repeat_ngram_size=2)\n",
    "\n",
    "  # \n",
    "  return outputs[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def get_answer(question):\n",
    "  pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "   # \n",
    "  prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "  prompt = prompt_template.format(query=question)\n",
    "\n",
    "  outputs = pipe(prompt, max_new_tokens=256, no_repeat_ngram_size=2)\n",
    "\n",
    "  # \n",
    "  return outputs[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Please analyze the two following provided code files in C or Java. Identify if they are semantically equal. 'Semantically equal' means two codes have the same meaning, that they have the same output given the same input.\n",
    "Here are three semantically equal examples:  \n",
    "The first example pair is\n",
    "``` Code 1\n",
    "double f(double M, double x) {\n",
    "    x = (M + x) / 2;\n",
    "    return x;\n",
    " }\n",
    "```\n",
    "``` Mutant Code 1\n",
    "double f(double M, double x) {\n",
    "    x = (M + x++ ) / 2;\n",
    "    return x;\n",
    " }\n",
    "```\n",
    "Yes. The two codes are semantically euqal because `M + x++` first does `M + x` and then `x++`. Therefore, `(M + x) / 2` is the same with `(M + x++) / 2`.\n",
    "\n",
    "\n",
    "The second example pair is \n",
    "``` Code 2\n",
    "double f(int x, int divisor){\n",
    "   return x*divisor;\n",
    "}\n",
    "```\n",
    "```Mutant Code 2\n",
    "double f(int x, int divisor){\n",
    "    return x*divisor++;\n",
    "}\n",
    "```\n",
    "Yes. The two codes are semantically euqal because `x*divisor` first does `x*divisor` and then `divisor++`. The two functions return the same values.\n",
    "\n",
    "\n",
    "The third example pair is \n",
    "``` Code 3\n",
    "int f(int a, int y, int x){\n",
    "    int p = a;\n",
    "    p = x + y;\n",
    "    return p;\n",
    "}\n",
    "```\n",
    "``` Mutant Code 3\n",
    "int f(int a, int y, int x){\n",
    "    int p = 1;\n",
    "    p = x + y;\n",
    "    return p;\n",
    "}\n",
    "```\n",
    "Yes. The two codes are semantically euqal because the local value `p` is re-assigned by `x+y`. There, the change `int p = 1;` will not affect the function.\n",
    "\n",
    "  \n",
    "\n",
    "Please identify if the two following codes are semantically equal. Please only answer `yes` or `no`. `yes` means they are semantically equal. `no` means they are not. \n",
    "Input :\n",
    "    ```Code \n",
    "// This is a mutant program.\n",
    "// Author : ysma\n",
    "\n",
    "public class Bisect\n",
    "{\n",
    "\n",
    "    double mEpsilon;\n",
    "\n",
    "    double mNumber;\n",
    "\n",
    "    double mResult;\n",
    "\n",
    "    public Bisect()\n",
    "    {\n",
    "    }\n",
    "\n",
    "    public  void setEpsilon( double epsilon )\n",
    "    {\n",
    "        this.mEpsilon = epsilon;\n",
    "    }\n",
    "\n",
    "    public  double sqrt( double N )\n",
    "    {\n",
    "        double x = N;\n",
    "        double M = N;\n",
    "        double m = 1;\n",
    "        double r = x;\n",
    "        double diff = x * x - N;\n",
    "        while (Math.abs(diff) > mEpsilon) {\n",
    "            if (diff < 0) {\n",
    "                m = x;\n",
    "                x = (M + x) / 2;\n",
    "            } else {\n",
    "                if (diff > 0) {\n",
    "                    M = x;\n",
    "                    x = (m + x) / 2;\n",
    "                }\n",
    "            }\n",
    "            diff = x * x - N;\n",
    "        }\n",
    "        r = x;\n",
    "        mResult = r;\n",
    "        return r;\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "```\n",
    "```Mutant Code \n",
    "// This is a mutant program.\n",
    "// Author : ysma\n",
    "\n",
    "public class Bisect\n",
    "{\n",
    "\n",
    "    double mEpsilon;\n",
    "\n",
    "    double mNumber;\n",
    "\n",
    "    double mResult;\n",
    "\n",
    "    public Bisect()\n",
    "    {\n",
    "    }\n",
    "\n",
    "    public  void setEpsilon( double epsilon )\n",
    "    {\n",
    "        this.mEpsilon = epsilon;\n",
    "    }\n",
    "\n",
    "    public  double sqrt( double N )\n",
    "    {\n",
    "        double x = N;\n",
    "        double M = N;\n",
    "        double m = 1;\n",
    "        double r = x;\n",
    "        double diff = x * x - N;\n",
    "        while (Math.abs(diff) > mEpsilon) {\n",
    "            if (diff < 0) {\n",
    "                m = x;\n",
    "                x = (M + x) / 2;\n",
    "            } else {\n",
    "                if (diff > 0) {\n",
    "                    M = x;\n",
    "                    x = (m + x) / 2;\n",
    "                }\n",
    "            }\n",
    "            diff = x * x - N;\n",
    "        }\n",
    "        r = x;\n",
    "        mResult = -r;\n",
    "        return r;\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "```\n",
    "The difference patch is \n",
    "\n",
    "    ```\n",
    "    @@ -42 +42 @@\n",
    "-        mResult = r;\n",
    "+        mResult = -r;\n",
    "\n",
    "    ```\n",
    "\n",
    "In this example, I return r, so 'mResult = -r' and 'mResult=r' didn't affect the output, so mResult is an unused variable\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = get_answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans2 = chat_with_model(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ans2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ans.split(\"[/INST]\\n#\")[-1]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_tmp(question):\n",
    "  return 'Yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "output_file = 'chatgpt4_zeroshot_predictions.xlsx'\n",
    "output_data = DataFrame(pd.read_excel(output_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, question in enumerate(questions):\n",
    "  if i<=110:\n",
    "    continue\n",
    "  with open(f'question/{i}.txt', 'r') as f:\n",
    "    question = f.read()\n",
    "  print('------------------------')\n",
    "  print(f\"question:{i}\")\n",
    "  print(question.split(\"```\\nThe difference patch is \")[-1])\n",
    "  content = file.iloc[i].copy()\n",
    "  ans = get_answer(question)\n",
    "  output = ans.split(\"[/INST]\\n#\")[-1]\n",
    "  print(output)\n",
    "\n",
    "  with open(f'codellama_zero/{i}.txt', 'w') as f:\n",
    "    f.write(output)\n",
    "\n",
    "\n",
    "for i, question in enumerate(questions_few):\n",
    "  if i<=110:\n",
    "    continue\n",
    "  with open(f'question/{i}.txt', 'r') as f:\n",
    "    question = f.read()\n",
    "  print('------------------------')\n",
    "  print(f\"question:{i}\")\n",
    "  print(question.split(\"```\\nThe difference patch is \")[-1])\n",
    "  content = file_few.iloc[i].copy()\n",
    "  ans = get_answer(question)\n",
    "  output = ans.split(\"[/INST]\\n#\")[-1]\n",
    "  print(output)\n",
    "\n",
    "  with open(f'codellama/{i}.txt', 'w') as f:\n",
    "    f.write(output)\n",
    "\n",
    "    \n",
    "\n",
    "  # print(ans)\n",
    "  # content['answer'] = ans\n",
    "\n",
    "  # file.iloc[i] = content\n",
    "  # file.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3974698/801747223.py:25: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'false' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[i, 'isCorrect'] = np.where(df.loc[i, 'label'] == df.loc[i, 'predict'], 'true', 'false')\n"
     ]
    }
   ],
   "source": [
    "# starchat\n",
    "#  Excel \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_excel('chatgpt4_zeroshot_predictions.xlsx')\n",
    "for i in range(200):\n",
    "    try:\n",
    "        with open(f'zeroshot/{i}.txt') as f:\n",
    "          content = f.read()\n",
    "          #  content  \"no\" \n",
    "          if content.strip().lower().startswith(\"no\"):\n",
    "              predict = 0\n",
    "          else:\n",
    "              predict = 1\n",
    "    except:\n",
    "       content = 'no'\n",
    "\n",
    "    # predict = 0\n",
    "\n",
    "    #  predict  answer\n",
    "    df.loc[i, 'predict'] = predict\n",
    "    df.loc[i, 'answer'] = content\n",
    "\n",
    "    #  label  predict\n",
    "    df.loc[i, 'isCorrect'] = np.where(df.loc[i, 'label'] == df.loc[i, 'predict'], 'true', 'false')\n",
    "  \n",
    "#  Excel \n",
    "df.to_excel('chatgpt4_zeroshot_predictions.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
